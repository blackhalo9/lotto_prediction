{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c709f33-1bfc-4a02-86f6-cd5c76467514",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESKO\\PYWORK\\NHITS\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "os.chdir(\"/Users/ESKO/PYWORK/NHITS\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1130c92b-7119-401d-8c4e-99b9474f1fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray import tune\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.auto import AutoTFT\n",
    "from neuralforecast.models import NBEATS, NBEATSx, NHITS, TFT\n",
    "from neuralforecast.losses.pytorch import MQLoss, DistributionLoss, GMM, PMM\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.losses.pytorch import MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e974b-728b-448b-aca0-03a9c91b7fff",
   "metadata": {},
   "source": [
    "## <기준날짜 및 예측 기간 설정>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f91c564f-7439-4f72-9730-ed3e3ccdd549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff_date = '2021-11-30' ## len = 65\n",
    "test_period = 12  ## 예측 기간 (월)\n",
    "corr_rate = 0.9 ## Lag corr rate\n",
    "future_variable_limit = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60de4ab8-1f14-4139-951e-77f82ba87471",
   "metadata": {},
   "source": [
    "## <원 data set>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "01ff22bb-7a0d-48ea-9ba3-2c3531f1361f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/metal_pst.csv')\n",
    "df.rename(columns={'date':'ds'},inplace=True)\n",
    "df.rename(columns={'li2co3_cif_fast_kg_spot_exchng':'y'},inplace=True)\n",
    "df['ds'] = pd.to_datetime(df['ds'], format='%Y-%m-%d')\n",
    "\n",
    "df_tgt = df.iloc[:,3:]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_tgt)\n",
    "arr = scaler.transform(df_tgt)\n",
    "df_tmp = pd.DataFrame(arr, columns=df_tgt.columns)\n",
    "df = pd.concat([df.iloc[:,:3],df_tmp], axis=1)\n",
    "\n",
    "raw_y_set_resampled = df.copy()\n",
    "raw_y_set_resampled.rename(columns={'ds':'date'},inplace=True)\n",
    "raw_y_set_resampled.rename(columns={'lioh_cif_fast_kg_spot_exchng':'y2'},inplace=True)\n",
    "del raw_y_set_resampled['unique_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f5486-9f68-4806-beb7-22fa9219aad0",
   "metadata": {},
   "source": [
    "## Feature shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f178475b-3d21-42b8-a56a-9fe5a627349f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.8 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "raw_y_set_resampled = raw_y_set_resampled[raw_y_set_resampled.date<=cutoff_date]\n",
    "raw_y_set_shifted_merged = raw_y_set_resampled['date']\n",
    "\n",
    "for i in range(len(raw_y_set_resampled.columns)):\n",
    "    if i == len(raw_y_set_resampled.columns) - 1:\n",
    "        break\n",
    "    raw_y_set_shifted = raw_y_set_resampled.iloc[:,[0,i+1]].dropna()\n",
    "    for j in range(test_period-1,test_period):\n",
    "        raw_y_set_shifted = pd.concat([raw_y_set_shifted, (raw_y_set_shifted.iloc[:,1].shift(j+1).rename(raw_y_set_resampled.columns[i+1] + '_' +str(j+1)))],axis=1)\n",
    "        raw_y_set_shifted_merged = pd.merge(raw_y_set_shifted_merged, raw_y_set_shifted, how= 'outer') \n",
    "        if j == len(raw_y_set_shifted.iloc[:,1]) -2 :\n",
    "            break\n",
    "# raw_y_set_shifted_merged.to_csv('./data/raw_y_set_shifted_merged.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889fa8d-cdb7-4a0c-a705-894d1746bdb7",
   "metadata": {},
   "source": [
    "#### 해당 기간에서의 lag corr best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "637d45e9-c3d2-4965-af8f-5553849d1f41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_y_set_shifted_merged_corr = raw_y_set_shifted_merged.corr()\n",
    "raw_y_set_shifted_merged_filtered = raw_y_set_shifted_merged_corr[raw_y_set_resampled.columns.drop('date')]\n",
    "raw_y_set_shifted_merged_filtered = raw_y_set_shifted_merged_filtered.loc[raw_y_set_shifted_merged_filtered.index != 'y']\n",
    "raw_y_set_shifted_merged_filtered = raw_y_set_shifted_merged_filtered.loc[raw_y_set_shifted_merged_filtered.index != 'y2']\n",
    "raw_y_set_shifted_merged_filtered_y = raw_y_set_shifted_merged_filtered.iloc[:,:1]\n",
    "raw_y_set_shifted_merged_filtered_upto_corr_rate = raw_y_set_shifted_merged_filtered_y[raw_y_set_shifted_merged_filtered_y.y >= corr_rate]\n",
    "raw_y_set_shifted_merged_filtered_upto_corr_rate = raw_y_set_shifted_merged_filtered_upto_corr_rate[raw_y_set_shifted_merged_filtered_upto_corr_rate.index.str.contains(f'_{test_period}')]\n",
    "raw_y_set_shifted_merged_filtered_upto_corr_rate = raw_y_set_shifted_merged_filtered_upto_corr_rate.sort_values('y',ascending=False)\n",
    "\n",
    "if len(raw_y_set_shifted_merged_filtered_upto_corr_rate) >= 20:\n",
    "    raw_y_set_shifted_merged_filtered_upto_corr_rate = raw_y_set_shifted_merged_filtered_upto_corr_rate[:future_variable_limit]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "futr_list = raw_y_set_shifted_merged_filtered_upto_corr_rate.index.values.tolist()\n",
    "futr_list = [word.replace(f\"_{test_period}\", \"\") for word in futr_list]\n",
    "\n",
    "# if 'y2' in futr_list:\n",
    "#     (futr_list).remove('y2')\n",
    "# if 'li2co395_am_kg' in futr_list:\n",
    "#     (futr_list).remove('li2co395_am_kg')\n",
    "# if 'li2co3_995_am_kg' in futr_list:\n",
    "#     (futr_list).remove('li2co3_995_am_kg')\n",
    "# if 'mn_mb_kg' in futr_list:\n",
    "#     (futr_list).remove('mn_mb_kg')\n",
    "# if 'al_lme_kg' in futr_list:\n",
    "#     (futr_list).remove('al_lme_kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f978b900-b299-430f-aa61-743ce133cd57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [y]\n",
       "Index: []"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_y_set_shifted_merged_filtered_upto_corr_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ec210705-f20b-4ed8-b3e7-97760f00cb17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "futr_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e2423-991f-4d6b-882c-50617a2a65e1",
   "metadata": {},
   "source": [
    "#### 해당 기간에서의 corr best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e40279d3-f9f2-410e-88ac-452b91a2f75c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_var_chcek = raw_y_set_resampled.iloc[:,1:]\n",
    "\n",
    "hist_var_corr = hist_var_chcek.corr()\n",
    "hist_var_corr_filtered = hist_var_corr.loc[hist_var_corr.index != 'y']\n",
    "hist_var_corr_filtered = hist_var_corr_filtered.loc[hist_var_corr_filtered.index != 'y2']\n",
    "hist_var_corr_filtered_y = hist_var_corr_filtered.iloc[:,:1]\n",
    "hist_var_corr_filtered_y_corr_rate = hist_var_corr_filtered_y[hist_var_corr_filtered_y.y >= corr_rate]\n",
    "hist_var_corr_filtered_y_only = hist_var_corr_filtered_y_corr_rate[hist_var_corr_filtered_y_corr_rate.index.str.contains('li2co3')==False]\n",
    "hist_var_corr_filtered_y_only = hist_var_corr_filtered_y_only[hist_var_corr_filtered_y_only.index.str.contains('lioh')==False]\n",
    "hist_var_corr_filtered_y_only = hist_var_corr_filtered_y_only.sort_values('y',ascending=False)\n",
    "hist_var_list = hist_var_corr_filtered_y_only.index.values.tolist()\n",
    "\n",
    "hist_var_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e986de-da6c-4b2d-9a92-6f5c9c8da01a",
   "metadata": {},
   "source": [
    "## scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37017b-fc8c-4ce2-93ff-3758d04817e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a209346-8f24-44ee-ba3b-a08f80614190",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### static DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5ce7033a-2234-4bac-9087-91daee3af55a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "static_df = pd.read_csv('/users/ESKO/PYWORK/NHITS/data/static_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fba8f2-a5ab-484b-b1ad-bda6ea55f7af",
   "metadata": {},
   "source": [
    "## <Training & Test>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135deb5-4427-45d2-b085-d0063d554f54",
   "metadata": {},
   "source": [
    "#### 날짜 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "525afb1e-6ec3-4108-9c23-95cc5565c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df.loc[df.ds < '2021-06-30']\n",
    "# test = df.loc[df.ds > '2021-06-30']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95faab-68f4-4566-9494-35d2b646a7d7",
   "metadata": {},
   "source": [
    "#### 날짜 별 len 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8548a462-acb8-462e-afcb-94c8c29af9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(df.loc[df.ds<'2021-06-30'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "837791bc-7eab-4e22-9501-5dda050fa80b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(Y_train_df)\n",
    "# len(Y_test_df)\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48e6ed-d091-4f7e-860d-32db64006c26",
   "metadata": {},
   "source": [
    "Test 용 DF 및 선행 변수 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e7947178-5017-4afa-99bb-de9843ac3061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_train_df = df[df.ds<=cutoff_date]\n",
    "Y_test_df = df[df.ds>cutoff_date]\n",
    "\n",
    "if len(Y_test_df) > 12:    \n",
    "    Y_test_df = Y_test_df[:12]\n",
    "else:\n",
    "    pass\n",
    "# Y_train_df = df[df.ds<df['ds'].values[len(df.loc[df.ds<=cutoff_date])]]\n",
    "# Y_test_df = df[df.ds>=df['ds'].values[len(df.loc[df.ds<=cutoff_date])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b6a349e3-e146-47f1-aae9-fd4ca29154e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-10-31</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds unique_id\n",
       "0  2021-12-31         A\n",
       "1  2022-01-31         A\n",
       "2  2022-02-28         A\n",
       "3  2022-03-31         A\n",
       "4  2022-04-30         A\n",
       "5  2022-05-31         A\n",
       "6  2022-06-30         A\n",
       "7  2022-07-31         A\n",
       "8  2022-08-31         A\n",
       "9  2022-09-30         A\n",
       "10 2022-10-31         A\n",
       "11 2022-11-30         A"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_date_series = pd.date_range(pd.date_range(cutoff_date,periods=2,freq='M')[1],periods=test_period,freq='M')\n",
    "future_date = pd.DataFrame(future_date_series,columns=['ds'])\n",
    "# future_date['unique_id'] = 'A'\n",
    "future_df_temp = pd.concat([raw_y_set_resampled.rename(columns={'date':'ds'}).iloc[-(test_period+len(Y_test_df)):,:],future_date])\n",
    "future_df_temp[futr_list] = future_df_temp[futr_list].shift(test_period)\n",
    "\n",
    "future_df_with_index = list(['ds','unique_id']) + futr_list\n",
    "future_df_with_index\n",
    "\n",
    "future_df_temp = future_df_temp[future_df_temp.columns[future_df_temp.columns.isin(future_df_with_index)]].reset_index(drop=True)\n",
    "future_df_temp.insert(1, 'unique_id', 'A')\n",
    "futr_df = future_df_temp.iloc[-test_period:,:].reset_index(drop=True)\n",
    "futr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b52225-6b2d-4ffb-939e-beab3e28281e",
   "metadata": {},
   "source": [
    "#### No 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e57c2c7e-3a33-4965-adec-88ef864f2038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# horizon = len(Y_test_df)\n",
    "\n",
    "# models = [\n",
    "#           # NBEATS(input_size=5 * horizon, gpus = 1, h=horizon, max_epochs=100),\n",
    "#           # NBEATSx(h=12, input_size=24,\n",
    "#           #       # loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "#           #       gpus=1,\n",
    "#           #       scaler_type='robust',\n",
    "#           #       stat_exog_list = None,  \n",
    "#           #       futr_exog_list = futr_list,\n",
    "#           #       max_steps=100,\n",
    "#           #       # val_check_steps=10,\n",
    "#           #       # early_stop_patience_steps=2\n",
    "#           #      ),\n",
    "#           NHITS(input_size=5 * horizon, \n",
    "#                 h=horizon+1,\n",
    "#                 gpus = 1,\n",
    "#                 stat_exog_list = None,\n",
    "#                 hist_exog_list=None,\n",
    "#                 futr_exog_list = None,\n",
    "#                 # futr_exog_list = futr_list,\n",
    "#                 # n_freq_downsample=[24, 12, 1],\n",
    "#                 # mlp_units = [[512, 512], [512, 512], [512, 512]],\n",
    "#                 # n_pool_kernel_size = [2, 2, 1],\n",
    "#                 # n_freq_downsample=[24, 12, 1],\n",
    "#                 scaler_type = 'standard',\n",
    "#                 learning_rate=1e-4,\n",
    "#                 pooling_mode = 'MaxPool1d',\n",
    "#                 activation='ReLU',\n",
    "#                 batch_size=128,\n",
    "#                 random_seed=42,\n",
    "#                 max_epochs=200\n",
    "#                ),\n",
    "#            # NHITS(\n",
    "#            #       h,\n",
    "#            #       input_size,\n",
    "#            #       futr_exog_list=None,\n",
    "#            #       hist_exog_list=None,\n",
    "#            #       stat_exog_list=None,\n",
    "#            #       stack_types: list = ['identity', 'identity', 'identity'],\n",
    "#            #       n_blocks: list = [1, 1, 1],\n",
    "#            #       mlp_units: list = [[512, 512], [512, 512], [512, 512]],\n",
    "#            #       n_pool_kernel_size: list = [2, 2, 1],\n",
    "#            #       n_freq_downsample: list = [4, 2, 1],\n",
    "#            #       pooling_mode: str = 'MaxPool1d',\n",
    "#            #       interpolation_mode: str = 'linear',\n",
    "#            #       dropout_prob_theta=0.0,\n",
    "#            #       activation='ReLU',\n",
    "#            #       loss=MAE(),\n",
    "#            #       learning_rate=0.001,\n",
    "#            #       batch_size=32,\n",
    "#            #       windows_batch_size: int = 1024,\n",
    "#            #       step_size: int = 1,\n",
    "#            #       scaler_type='identity',\n",
    "#            #       random_seed=1,\n",
    "#            #       num_workers_loader=0,\n",
    "#            #       drop_last_loader=False,\n",
    "#            #       **trainer_kwargs,\n",
    "    \n",
    "#           # TFT(h=12, \n",
    "#           #     input_size=5 * horizon,\n",
    "#           #     # hidden_size=20,\n",
    "#           #     #loss=DistributionLoss(distribution='Poisson', level=[80, 90]),\n",
    "#           #     #loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "#           #     #loss=DistributionLoss(distribution='StudentT', level=[80, 90]),\n",
    "#           #     learning_rate=0.005,\n",
    "#           #     stat_exog_list = None,\n",
    "#           #     futr_exog_list = futr_list,\n",
    "#           #     max_steps=100,\n",
    "#           #     gpus=1,\n",
    "#           #     # val_check_steps=10,\n",
    "#           #     # early_stop_patience_steps=10,\n",
    "#           #     scaler_type='robust',\n",
    "#           #     # windows_batch_size=None,\n",
    "#           #     enable_progress_bar=True),\n",
    "#              ]\n",
    "# nforecast_nv = NeuralForecast(models=models, freq='M')\n",
    "# nforecast_nv.fit(df=Y_train_df)\n",
    "# Y_hat_df_nv = nforecast_nv.predict(futr_df=futr_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "23a1910f-285e-457a-888e-8ca4fa041ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot predictions\n",
    "# fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "# Y_hat_df_nv = Y_test_df.merge(Y_hat_df_nv, how='left', on=['unique_id', 'ds'])\n",
    "# plot_df_nv = pd.concat([Y_train_df, Y_hat_df_nv]).set_index('ds')\n",
    "\n",
    "# plot_df_nv[['y', 'NHITS']].plot(ax=ax, linewidth=2, marker='o')\n",
    "# plt.axvline(cutoff_date, color='red')\n",
    "\n",
    "# ax.set_title(f'LI2CO3 Actual and Predicted Plot - Cutoff: {cutoff_date}', fontsize=22)\n",
    "# ax.set_ylabel('LI2CO3_PRICE', fontsize=20)\n",
    "# ax.set_xlabel('Date', fontsize=20)\n",
    "# ax.legend(prop={'size': 15})\n",
    "# ax.grid()\n",
    "\n",
    "# fig.savefig(f'./data/forecast_plot_{cutoff_date}_nv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "940332ca-281c-41c8-ac8f-2bc9646f7638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_df_nv[['y', 'NHITS']].to_csv(f'./data/forecasting_{cutoff_date}_nv.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6e49b-657e-48e0-b389-d96d77b28470",
   "metadata": {},
   "source": [
    "#### Yes 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d139885f-94df-4437-9a56-412123466be7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>y</th>\n",
       "      <th>lioh_cif_fast_kg_spot_exchng</th>\n",
       "      <th>Baltic Dry Index (BDI)</th>\n",
       "      <th>Howe Robinson Container Index</th>\n",
       "      <th>Shanghai Containerized Freight Index (SCFI)</th>\n",
       "      <th>옥수수 For Grain [미국(생산자 가격)] 현물</th>\n",
       "      <th>원당 (daily) [ISA] 현물</th>\n",
       "      <th>대두 [미국(생산자 가격)] 현물</th>\n",
       "      <th>...</th>\n",
       "      <th>Pilbara Mineral Limited_price_Open</th>\n",
       "      <th>Pilbara Mineral Limited_price_High</th>\n",
       "      <th>Pilbara Mineral Limited_price_Low</th>\n",
       "      <th>Pilbara Mineral Limited_price_Close</th>\n",
       "      <th>Pilbara Mineral Limited_price_Adj Close</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Open</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_High</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Low</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Close</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>A</td>\n",
       "      <td>33.043478</td>\n",
       "      <td>0.286579</td>\n",
       "      <td>1.582281</td>\n",
       "      <td>1.839908</td>\n",
       "      <td>1.084157</td>\n",
       "      <td>0.969759</td>\n",
       "      <td>1.115623</td>\n",
       "      <td>0.520883</td>\n",
       "      <td>...</td>\n",
       "      <td>1.766336</td>\n",
       "      <td>1.775690</td>\n",
       "      <td>1.776023</td>\n",
       "      <td>1.790538</td>\n",
       "      <td>1.704423</td>\n",
       "      <td>1.185124</td>\n",
       "      <td>1.187299</td>\n",
       "      <td>1.189690</td>\n",
       "      <td>1.197156</td>\n",
       "      <td>1.197156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>A</td>\n",
       "      <td>43.260870</td>\n",
       "      <td>0.631414</td>\n",
       "      <td>0.318707</td>\n",
       "      <td>1.983558</td>\n",
       "      <td>1.463396</td>\n",
       "      <td>1.048823</td>\n",
       "      <td>0.916574</td>\n",
       "      <td>0.779937</td>\n",
       "      <td>...</td>\n",
       "      <td>2.677508</td>\n",
       "      <td>2.680955</td>\n",
       "      <td>2.665481</td>\n",
       "      <td>2.680401</td>\n",
       "      <td>2.510212</td>\n",
       "      <td>1.610945</td>\n",
       "      <td>1.608097</td>\n",
       "      <td>1.585530</td>\n",
       "      <td>1.586450</td>\n",
       "      <td>1.586450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>A</td>\n",
       "      <td>56.425000</td>\n",
       "      <td>1.270279</td>\n",
       "      <td>0.406105</td>\n",
       "      <td>2.294724</td>\n",
       "      <td>1.435042</td>\n",
       "      <td>1.467862</td>\n",
       "      <td>0.798730</td>\n",
       "      <td>1.945680</td>\n",
       "      <td>...</td>\n",
       "      <td>2.197789</td>\n",
       "      <td>2.207543</td>\n",
       "      <td>2.190755</td>\n",
       "      <td>2.181831</td>\n",
       "      <td>2.058747</td>\n",
       "      <td>1.168835</td>\n",
       "      <td>1.174279</td>\n",
       "      <td>1.163390</td>\n",
       "      <td>1.167415</td>\n",
       "      <td>1.167415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>A</td>\n",
       "      <td>71.086957</td>\n",
       "      <td>2.126927</td>\n",
       "      <td>1.148209</td>\n",
       "      <td>2.612162</td>\n",
       "      <td>1.241856</td>\n",
       "      <td>1.831557</td>\n",
       "      <td>1.195636</td>\n",
       "      <td>2.269497</td>\n",
       "      <td>...</td>\n",
       "      <td>2.026895</td>\n",
       "      <td>2.013125</td>\n",
       "      <td>2.037362</td>\n",
       "      <td>2.013198</td>\n",
       "      <td>1.906047</td>\n",
       "      <td>1.471655</td>\n",
       "      <td>1.468981</td>\n",
       "      <td>1.476677</td>\n",
       "      <td>1.477051</td>\n",
       "      <td>1.477051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>A</td>\n",
       "      <td>75.804348</td>\n",
       "      <td>2.687810</td>\n",
       "      <td>0.860751</td>\n",
       "      <td>2.556884</td>\n",
       "      <td>1.425830</td>\n",
       "      <td>2.242691</td>\n",
       "      <td>1.391677</td>\n",
       "      <td>2.722841</td>\n",
       "      <td>...</td>\n",
       "      <td>2.170896</td>\n",
       "      <td>2.152358</td>\n",
       "      <td>2.152352</td>\n",
       "      <td>2.132638</td>\n",
       "      <td>2.410643</td>\n",
       "      <td>2.259286</td>\n",
       "      <td>2.271036</td>\n",
       "      <td>2.255625</td>\n",
       "      <td>2.261700</td>\n",
       "      <td>2.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>A</td>\n",
       "      <td>74.295455</td>\n",
       "      <td>2.621695</td>\n",
       "      <td>1.713126</td>\n",
       "      <td>2.501607</td>\n",
       "      <td>1.892086</td>\n",
       "      <td>2.385006</td>\n",
       "      <td>1.329234</td>\n",
       "      <td>3.305713</td>\n",
       "      <td>...</td>\n",
       "      <td>1.814178</td>\n",
       "      <td>1.807191</td>\n",
       "      <td>1.810962</td>\n",
       "      <td>1.808275</td>\n",
       "      <td>2.410643</td>\n",
       "      <td>2.106495</td>\n",
       "      <td>2.106820</td>\n",
       "      <td>2.111163</td>\n",
       "      <td>2.116623</td>\n",
       "      <td>2.116623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>A</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>2.389233</td>\n",
       "      <td>1.060177</td>\n",
       "      <td>2.515599</td>\n",
       "      <td>2.230336</td>\n",
       "      <td>2.471976</td>\n",
       "      <td>1.161657</td>\n",
       "      <td>3.111422</td>\n",
       "      <td>...</td>\n",
       "      <td>1.310465</td>\n",
       "      <td>1.314768</td>\n",
       "      <td>1.273957</td>\n",
       "      <td>1.284862</td>\n",
       "      <td>2.410643</td>\n",
       "      <td>1.628496</td>\n",
       "      <td>1.623594</td>\n",
       "      <td>1.607981</td>\n",
       "      <td>1.608344</td>\n",
       "      <td>1.608344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>A</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>2.389233</td>\n",
       "      <td>0.691597</td>\n",
       "      <td>2.439292</td>\n",
       "      <td>2.526593</td>\n",
       "      <td>2.377100</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>2.852368</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464141</td>\n",
       "      <td>1.460176</td>\n",
       "      <td>1.481963</td>\n",
       "      <td>1.473009</td>\n",
       "      <td>2.410643</td>\n",
       "      <td>1.436494</td>\n",
       "      <td>1.434052</td>\n",
       "      <td>1.439497</td>\n",
       "      <td>1.441548</td>\n",
       "      <td>1.441548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>A</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>2.420773</td>\n",
       "      <td>-0.092744</td>\n",
       "      <td>2.119559</td>\n",
       "      <td>2.773368</td>\n",
       "      <td>2.369193</td>\n",
       "      <td>0.793818</td>\n",
       "      <td>2.593314</td>\n",
       "      <td>...</td>\n",
       "      <td>2.219001</td>\n",
       "      <td>2.233095</td>\n",
       "      <td>2.245839</td>\n",
       "      <td>2.264053</td>\n",
       "      <td>2.410643</td>\n",
       "      <td>2.144853</td>\n",
       "      <td>2.155770</td>\n",
       "      <td>2.181382</td>\n",
       "      <td>2.184215</td>\n",
       "      <td>2.184215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>A</td>\n",
       "      <td>74.047619</td>\n",
       "      <td>2.507937</td>\n",
       "      <td>-0.004062</td>\n",
       "      <td>1.188816</td>\n",
       "      <td>3.024986</td>\n",
       "      <td>2.250597</td>\n",
       "      <td>0.750690</td>\n",
       "      <td>1.621862</td>\n",
       "      <td>...</td>\n",
       "      <td>2.107414</td>\n",
       "      <td>2.092161</td>\n",
       "      <td>2.084336</td>\n",
       "      <td>2.084871</td>\n",
       "      <td>1.970948</td>\n",
       "      <td>2.828439</td>\n",
       "      <td>2.823884</td>\n",
       "      <td>2.834908</td>\n",
       "      <td>2.836251</td>\n",
       "      <td>2.836251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2022-10-31</td>\n",
       "      <td>A</td>\n",
       "      <td>77.673913</td>\n",
       "      <td>2.664680</td>\n",
       "      <td>0.381286</td>\n",
       "      <td>0.204375</td>\n",
       "      <td>3.036207</td>\n",
       "      <td>1.784119</td>\n",
       "      <td>0.691518</td>\n",
       "      <td>1.427572</td>\n",
       "      <td>...</td>\n",
       "      <td>2.352264</td>\n",
       "      <td>2.359441</td>\n",
       "      <td>2.354164</td>\n",
       "      <td>2.362142</td>\n",
       "      <td>2.222022</td>\n",
       "      <td>2.765597</td>\n",
       "      <td>2.782167</td>\n",
       "      <td>2.764528</td>\n",
       "      <td>2.761120</td>\n",
       "      <td>2.761120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>A</td>\n",
       "      <td>80.818182</td>\n",
       "      <td>2.828879</td>\n",
       "      <td>-0.226024</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>3.004479</td>\n",
       "      <td>1.784119</td>\n",
       "      <td>1.005474</td>\n",
       "      <td>1.621862</td>\n",
       "      <td>...</td>\n",
       "      <td>2.433279</td>\n",
       "      <td>2.443712</td>\n",
       "      <td>2.455263</td>\n",
       "      <td>2.452607</td>\n",
       "      <td>2.303940</td>\n",
       "      <td>2.830058</td>\n",
       "      <td>2.817705</td>\n",
       "      <td>2.816214</td>\n",
       "      <td>2.795982</td>\n",
       "      <td>2.795982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 925 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds unique_id          y  lioh_cif_fast_kg_spot_exchng  \\\n",
       "71 2021-12-31         A  33.043478                      0.286579   \n",
       "72 2022-01-31         A  43.260870                      0.631414   \n",
       "73 2022-02-28         A  56.425000                      1.270279   \n",
       "74 2022-03-31         A  71.086957                      2.126927   \n",
       "75 2022-04-30         A  75.804348                      2.687810   \n",
       "76 2022-05-31         A  74.295455                      2.621695   \n",
       "77 2022-06-30         A  73.000000                      2.389233   \n",
       "78 2022-07-31         A  73.000000                      2.389233   \n",
       "79 2022-08-31         A  73.000000                      2.420773   \n",
       "80 2022-09-30         A  74.047619                      2.507937   \n",
       "81 2022-10-31         A  77.673913                      2.664680   \n",
       "82 2022-11-30         A  80.818182                      2.828879   \n",
       "\n",
       "    Baltic Dry Index (BDI)  Howe Robinson Container Index  \\\n",
       "71                1.582281                       1.839908   \n",
       "72                0.318707                       1.983558   \n",
       "73                0.406105                       2.294724   \n",
       "74                1.148209                       2.612162   \n",
       "75                0.860751                       2.556884   \n",
       "76                1.713126                       2.501607   \n",
       "77                1.060177                       2.515599   \n",
       "78                0.691597                       2.439292   \n",
       "79               -0.092744                       2.119559   \n",
       "80               -0.004062                       1.188816   \n",
       "81                0.381286                       0.204375   \n",
       "82               -0.226024                      -0.008117   \n",
       "\n",
       "    Shanghai Containerized Freight Index (SCFI)  \\\n",
       "71                                     1.084157   \n",
       "72                                     1.463396   \n",
       "73                                     1.435042   \n",
       "74                                     1.241856   \n",
       "75                                     1.425830   \n",
       "76                                     1.892086   \n",
       "77                                     2.230336   \n",
       "78                                     2.526593   \n",
       "79                                     2.773368   \n",
       "80                                     3.024986   \n",
       "81                                     3.036207   \n",
       "82                                     3.004479   \n",
       "\n",
       "    옥수수 For Grain [미국(생산자 가격)] 현물  원당 (daily) [ISA] 현물  대두 [미국(생산자 가격)] 현물  \\\n",
       "71                       0.969759             1.115623            0.520883   \n",
       "72                       1.048823             0.916574            0.779937   \n",
       "73                       1.467862             0.798730            1.945680   \n",
       "74                       1.831557             1.195636            2.269497   \n",
       "75                       2.242691             1.391677            2.722841   \n",
       "76                       2.385006             1.329234            3.305713   \n",
       "77                       2.471976             1.161657            3.111422   \n",
       "78                       2.377100             0.921668            2.852368   \n",
       "79                       2.369193             0.793818            2.593314   \n",
       "80                       2.250597             0.750690            1.621862   \n",
       "81                       1.784119             0.691518            1.427572   \n",
       "82                       1.784119             1.005474            1.621862   \n",
       "\n",
       "    ...  Pilbara Mineral Limited_price_Open  \\\n",
       "71  ...                            1.766336   \n",
       "72  ...                            2.677508   \n",
       "73  ...                            2.197789   \n",
       "74  ...                            2.026895   \n",
       "75  ...                            2.170896   \n",
       "76  ...                            1.814178   \n",
       "77  ...                            1.310465   \n",
       "78  ...                            1.464141   \n",
       "79  ...                            2.219001   \n",
       "80  ...                            2.107414   \n",
       "81  ...                            2.352264   \n",
       "82  ...                            2.433279   \n",
       "\n",
       "    Pilbara Mineral Limited_price_High  Pilbara Mineral Limited_price_Low  \\\n",
       "71                            1.775690                           1.776023   \n",
       "72                            2.680955                           2.665481   \n",
       "73                            2.207543                           2.190755   \n",
       "74                            2.013125                           2.037362   \n",
       "75                            2.152358                           2.152352   \n",
       "76                            1.807191                           1.810962   \n",
       "77                            1.314768                           1.273957   \n",
       "78                            1.460176                           1.481963   \n",
       "79                            2.233095                           2.245839   \n",
       "80                            2.092161                           2.084336   \n",
       "81                            2.359441                           2.354164   \n",
       "82                            2.443712                           2.455263   \n",
       "\n",
       "    Pilbara Mineral Limited_price_Close  \\\n",
       "71                             1.790538   \n",
       "72                             2.680401   \n",
       "73                             2.181831   \n",
       "74                             2.013198   \n",
       "75                             2.132638   \n",
       "76                             1.808275   \n",
       "77                             1.284862   \n",
       "78                             1.473009   \n",
       "79                             2.264053   \n",
       "80                             2.084871   \n",
       "81                             2.362142   \n",
       "82                             2.452607   \n",
       "\n",
       "    Pilbara Mineral Limited_price_Adj Close  \\\n",
       "71                                 1.704423   \n",
       "72                                 2.510212   \n",
       "73                                 2.058747   \n",
       "74                                 1.906047   \n",
       "75                                 2.410643   \n",
       "76                                 2.410643   \n",
       "77                                 2.410643   \n",
       "78                                 2.410643   \n",
       "79                                 2.410643   \n",
       "80                                 1.970948   \n",
       "81                                 2.222022   \n",
       "82                                 2.303940   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Open  Allkem Limited (AKE.AX)_price_High  \\\n",
       "71                            1.185124                            1.187299   \n",
       "72                            1.610945                            1.608097   \n",
       "73                            1.168835                            1.174279   \n",
       "74                            1.471655                            1.468981   \n",
       "75                            2.259286                            2.271036   \n",
       "76                            2.106495                            2.106820   \n",
       "77                            1.628496                            1.623594   \n",
       "78                            1.436494                            1.434052   \n",
       "79                            2.144853                            2.155770   \n",
       "80                            2.828439                            2.823884   \n",
       "81                            2.765597                            2.782167   \n",
       "82                            2.830058                            2.817705   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Low  Allkem Limited (AKE.AX)_price_Close  \\\n",
       "71                           1.189690                             1.197156   \n",
       "72                           1.585530                             1.586450   \n",
       "73                           1.163390                             1.167415   \n",
       "74                           1.476677                             1.477051   \n",
       "75                           2.255625                             2.261700   \n",
       "76                           2.111163                             2.116623   \n",
       "77                           1.607981                             1.608344   \n",
       "78                           1.439497                             1.441548   \n",
       "79                           2.181382                             2.184215   \n",
       "80                           2.834908                             2.836251   \n",
       "81                           2.764528                             2.761120   \n",
       "82                           2.816214                             2.795982   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Adj Close  \n",
       "71                                 1.197156  \n",
       "72                                 1.586450  \n",
       "73                                 1.167415  \n",
       "74                                 1.477051  \n",
       "75                                 2.261700  \n",
       "76                                 2.116623  \n",
       "77                                 1.608344  \n",
       "78                                 1.441548  \n",
       "79                                 2.184215  \n",
       "80                                 2.836251  \n",
       "81                                 2.761120  \n",
       "82                                 2.795982  \n",
       "\n",
       "[12 rows x 925 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "16b3c015-bcd6-48b4-a847-d414d3e093e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 6.00 GiB total capacity; 4.91 GiB already allocated; 0 bytes free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 91\u001b[0m\n\u001b[0;32m     12\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m           NBEATS(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m horizon, gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, h\u001b[38;5;241m=\u001b[39mhorizon, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m),\n\u001b[0;32m     14\u001b[0m           NBEATSx(h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m               enable_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     89\u001b[0m              ]\n\u001b[0;32m     90\u001b[0m nforecast \u001b[38;5;241m=\u001b[39m NeuralForecast(models\u001b[38;5;241m=\u001b[39mmodels, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 91\u001b[0m \u001b[43mnforecast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m Y_hat_df \u001b[38;5;241m=\u001b[39m nforecast\u001b[38;5;241m.\u001b[39mpredict(futr_df\u001b[38;5;241m=\u001b[39mfutr_df)\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\neuralforecast\\core.py:157\u001b[0m, in \u001b[0;36mNeuralForecast.fit\u001b[1;34m(self, df, static_df, val_size, sort_df, verbose)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# train + validation\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[1;32m--> 157\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# train with the full dataset\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\neuralforecast\\common\\_base_windows.py:493\u001b[0m, in \u001b[0;36mBaseWindows.fit\u001b[1;34m(self, dataset, val_size, test_size)\u001b[0m\n\u001b[0;32m    485\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m TimeSeriesDataModule(\n\u001b[0;32m    486\u001b[0m     dataset,\n\u001b[0;32m    487\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    488\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers_loader,\n\u001b[0;32m    489\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_last_loader,\n\u001b[0;32m    490\u001b[0m )\n\u001b[0;32m    492\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer_kwargs)\n\u001b[1;32m--> 493\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 770\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 723\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    807\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    809\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    810\u001b[0m )\n\u001b[1;32m--> 811\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m-> 1236\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1238\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1353\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:266\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(\n\u001b[0;32m    263\u001b[0m     dataloader, batch_to_device\u001b[38;5;241m=\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_strategy_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataloader_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    264\u001b[0m )\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:208\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 208\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m     87\u001b[0m     optimizers \u001b[38;5;241m=\u001b[39m _get_active_optimizers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[1;32m---> 88\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_loop\u001b[38;5;241m.\u001b[39mrun(split_batch, batch_idx)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:203\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[1;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madvance\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_optimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim_progress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_position\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;66;03m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;66;03m# would be skipped otherwise\u001b[39;00m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_idx] \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39masdict()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:256\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[1;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[0;32m    249\u001b[0m         closure()\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:369\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[1;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_tpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTPUAccelerator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_native_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mAMPType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNATIVE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_lbfgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_lbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1595\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1592\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1595\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py:1646\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1566\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1573\u001b[0m     using_lbfgs: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;124;03m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m \u001b[38;5;124;03m    each optimizer.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1644\u001b[0m \n\u001b[0;32m   1645\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1646\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:168\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:193\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual optimizer step.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    **kwargs: Any extra arguments to ``optimizer.step``\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:155\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m    154\u001b[0m     closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\optim\\adam.py:183\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 183\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    186\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:140\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    129\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:148\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:134\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 134\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:427\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[1;34m(self, split_batch, batch_idx, opt_idx)\u001b[0m\n\u001b[0;32m    422\u001b[0m step_kwargs \u001b[38;5;241m=\u001b[39m _build_training_step_kwargs(\n\u001b[0;32m    423\u001b[0m     lightning_module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers, split_batch, batch_idx, opt_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hiddens\n\u001b[0;32m    424\u001b[0m )\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 427\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[0;32m    430\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, training_step_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1765\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1765\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1768\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:333\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The actual training step.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.training_step` for more details\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\neuralforecast\\common\\_base_windows.py:317\u001b[0m, in \u001b[0;36mBaseWindows.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# Create and normalize windows [Ws, L+H, C]\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalization(windows\u001b[38;5;241m=\u001b[39mwindows)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# Parse windows\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\neuralforecast\\common\\_base_windows.py:161\u001b[0m, in \u001b[0;36mBaseWindows._create_windows\u001b[1;34m(self, batch, step)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindows_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     w_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\n\u001b[0;32m    157\u001b[0m         n_windows,\n\u001b[0;32m    158\u001b[0m         size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindows_batch_size,\n\u001b[0;32m    159\u001b[0m         replace\u001b[38;5;241m=\u001b[39m(n_windows \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindows_batch_size),\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m     windows \u001b[38;5;241m=\u001b[39m \u001b[43mwindows\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw_idxs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m static \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m         static \u001b[38;5;241m=\u001b[39m static[w_idxs]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 6.00 GiB total capacity; 4.91 GiB already allocated; 0 bytes free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "if len(Y_test_df) > 0:\n",
    "    horizon = len(Y_test_df)\n",
    "else:\n",
    "    horizon = len(futr_df)\n",
    "    \n",
    "    \n",
    "if cutoff_date <= '2021-12-31':\n",
    "    flexible = None\n",
    "else:\n",
    "    flexible = futr_df\n",
    "\n",
    "models = [\n",
    "          NBEATS(input_size=5 * horizon, gpus = 1, h=horizon, max_epochs=100),\n",
    "          NBEATSx(h=12, input_size=24,\n",
    "                # loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "                gpus=1,\n",
    "                scaler_type='robust',\n",
    "                stat_exog_list = None,  \n",
    "                futr_exog_list = flexible,\n",
    "                max_steps=100,\n",
    "                learning_rate=1e-4,\n",
    "                # val_check_steps=10,\n",
    "                # early_stop_patience_steps=2\n",
    "               ),\n",
    "          NHITS(input_size=5 * horizon, \n",
    "                h=horizon,\n",
    "                gpus = 1,\n",
    "                # stat_exog_list = None,\n",
    "                # hist_exog_list=hist_var_list,\n",
    "                futr_exog_list = flexible,\n",
    "                # n_blocks = [1, 1, 1],\n",
    "                mlp_units = [[512, 512], [512, 512], [512, 512]],\n",
    "                n_pool_kernel_size = [2, 2, 1],\n",
    "                # n_freq_downsample=[24, 12, 1],\n",
    "                scaler_type = 'robust',\n",
    "                learning_rate=1e-4,\n",
    "                pooling_mode = 'MaxPool1d',\n",
    "                activation='ReLU',\n",
    "                batch_size=128,\n",
    "                random_seed=42,\n",
    "                max_epochs=100\n",
    "               ),\n",
    "           # NHITS(\n",
    "           #       h,\n",
    "           #       input_size,\n",
    "           #       futr_exog_list=None,\n",
    "           #       hist_exog_list=None,\n",
    "           #       stat_exog_list=None,\n",
    "           #       stack_types: list = ['identity', 'identity', 'identity'],\n",
    "           #       n_blocks: list = [1, 1, 1],\n",
    "           #       mlp_units: list = [[512, 512], [512, 512], [512, 512]],\n",
    "           #       n_pool_kernel_size: list = [2, 2, 1],\n",
    "           #       n_freq_downsample: list = [4, 2, 1],\n",
    "           #       pooling_mode: str = 'MaxPool1d',\n",
    "           #       interpolation_mode: str = 'linear',\n",
    "           #       dropout_prob_theta=0.0,\n",
    "           #       activation='ReLU',\n",
    "           #       loss=MAE(),\n",
    "           #       learning_rate=0.001,\n",
    "           #       batch_size=32,\n",
    "           #       windows_batch_size: int = 1024,\n",
    "           #       step_size: int = 1,\n",
    "           #       scaler_type='identity',\n",
    "           #       random_seed=1,\n",
    "           #       num_workers_loader=0,\n",
    "           #       drop_last_loader=False,\n",
    "           #       **trainer_kwargs,\n",
    "    \n",
    "          TFT(h=12, \n",
    "              input_size=5 * horizon,\n",
    "              # hidden_size=20,\n",
    "              #loss=DistributionLoss(distribution='Poisson', level=[80, 90]),\n",
    "              #loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "              #loss=DistributionLoss(distribution='StudentT', level=[80, 90]),\n",
    "              hidden_size=512,\n",
    "              n_head=4, \n",
    "              attn_dropout=0.0, \n",
    "              dropout=0.1,\n",
    "              learning_rate=1e-4,\n",
    "              stat_exog_list = None,\n",
    "              futr_exog_list = flexible,\n",
    "              max_steps=100,\n",
    "              gpus=1,\n",
    "              # val_check_steps=10,\n",
    "              # early_stop_patience_steps=10,\n",
    "              scaler_type='robust',\n",
    "              # windows_batch_size=None,\n",
    "              enable_progress_bar=True),\n",
    "             ]\n",
    "nforecast = NeuralForecast(models=models, freq='M')\n",
    "nforecast.fit(df=Y_train_df)\n",
    "Y_hat_df = nforecast.predict(futr_df=futr_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30866cf9-6543-4f6e-802c-febe741b3492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "Y_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\n",
    "plot_df = pd.concat([Y_train_df, Y_hat_df]).set_index('ds')\n",
    "\n",
    "plot_df[['y','NBEATS', 'NHITS','TFT']].plot(ax=ax, linewidth=2, marker='o')\n",
    "\n",
    "plt.axvline(cutoff_date, color='red')\n",
    "ax.set_title(f'LI2CO3 Actual and Predicted Plot - Cutoff: {cutoff_date}', fontsize=22)\n",
    "ax.set_ylabel('LI2CO3_PRICE', fontsize=20)\n",
    "ax.set_xlabel('Date', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()\n",
    "\n",
    "# fig1.savefig(f'./data/forecasting_{cutoff_date}.png', dpi=100)\n",
    "fig.savefig(f'./data/forecast_plot_{cutoff_date}_wv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724b4e1-2d8c-4f3e-a5f8-433b9850c9d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_df[['y', 'NHITS']].to_csv(f'./data/forecast_plot_{cutoff_date}_wv.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a7b7d-0c76-4d10-ba18-567112c03448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07958c76-eac0-4543-bbbd-9ec3c6f1cf43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd1452-35e9-445c-a7aa-f694d62b3800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d11ff9-4c26-415e-8876-f9f84ea3df5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
