{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c709f33-1bfc-4a02-86f6-cd5c76467514",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESKO\\anaconda3\\envs\\dl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESKO\\PYWORK\\NHITS\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "os.chdir(\"/Users/ESKO/PYWORK/NHITS\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1130c92b-7119-401d-8c4e-99b9474f1fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray import tune\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.auto import AutoTFT\n",
    "from neuralforecast.models import NBEATS, NBEATSx, NHITS, TFT, LSTM\n",
    "from neuralforecast.losses.pytorch import MQLoss, DistributionLoss, GMM, PMM\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.losses.pytorch import MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e974b-728b-448b-aca0-03a9c91b7fff",
   "metadata": {},
   "source": [
    "## <기준날짜 및 예측 기간 설정>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f91c564f-7439-4f72-9730-ed3e3ccdd549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff_date = '2022-06-30' ## len = 65\n",
    "test_period = 6  ## 예측 기간 (월)\n",
    "corr_rate = 0.7 ## Lag corr rate\n",
    "hist_corr_rate = 0.9 ## Lag corr rate\n",
    "future_variable_limit = 2\n",
    "hist_variable_limit = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60de4ab8-1f14-4139-951e-77f82ba87471",
   "metadata": {},
   "source": [
    "## <원 data set>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ff22bb-7a0d-48ea-9ba3-2c3531f1361f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/metal_pst.csv')\n",
    "df.rename(columns={'date':'ds'},inplace=True)\n",
    "df.rename(columns={'li2co3_cif_fast_kg_spot_exchng':'y'},inplace=True)\n",
    "df['ds'] = pd.to_datetime(df['ds'], format='%Y-%m-%d')\n",
    "\n",
    "df_tgt = df.iloc[:,3:]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_tgt)\n",
    "arr = scaler.transform(df_tgt)\n",
    "df_tmp = pd.DataFrame(arr, columns=df_tgt.columns)\n",
    "df = pd.concat([df.iloc[:,:3],df_tmp], axis=1)\n",
    "\n",
    "raw_y_set_resampled = df.copy()\n",
    "raw_y_set_resampled.rename(columns={'ds':'date'},inplace=True)\n",
    "raw_y_set_resampled.rename(columns={'lioh_cif_fast_kg_spot_exchng':'y2'},inplace=True)\n",
    "del raw_y_set_resampled['unique_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f5486-9f68-4806-beb7-22fa9219aad0",
   "metadata": {},
   "source": [
    "## Feature shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f178475b-3d21-42b8-a56a-9fe5a627349f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "raw_y_set_resampled = raw_y_set_resampled[raw_y_set_resampled.date<=cutoff_date]\n",
    "raw_y_set_shifted_merged = raw_y_set_resampled['date']\n",
    "\n",
    "for i in range(len(raw_y_set_resampled.columns)):\n",
    "    if i == len(raw_y_set_resampled.columns) - 1:\n",
    "        break\n",
    "    raw_y_set_shifted = raw_y_set_resampled.iloc[:,[0,i+1]].dropna()\n",
    "    for j in range(test_period-1,test_period):\n",
    "        raw_y_set_shifted = pd.concat([raw_y_set_shifted, (raw_y_set_shifted.iloc[:,1].shift(j+1).rename(raw_y_set_resampled.columns[i+1] + '_' +str(j+1)+ '_'))],axis=1)\n",
    "        raw_y_set_shifted_merged = pd.merge(raw_y_set_shifted_merged, raw_y_set_shifted, how= 'outer') \n",
    "        if j == len(raw_y_set_shifted.iloc[:,1]) -2 :\n",
    "            break\n",
    "# raw_y_set_shifted_merged.to_csv('./data/raw_y_set_shifted_merged.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889fa8d-cdb7-4a0c-a705-894d1746bdb7",
   "metadata": {},
   "source": [
    "#### 해당 기간에서의 lag corr best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ca6511-1107-4780-8b99-0a65cb197401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>y</th>\n",
       "      <th>y_6_</th>\n",
       "      <th>y2</th>\n",
       "      <th>y2_6_</th>\n",
       "      <th>Baltic Dry Index (BDI)</th>\n",
       "      <th>Baltic Dry Index (BDI)_6_</th>\n",
       "      <th>Howe Robinson Container Index</th>\n",
       "      <th>Howe Robinson Container Index_6_</th>\n",
       "      <th>Shanghai Containerized Freight Index (SCFI)</th>\n",
       "      <th>...</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Open</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Open_6_</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_High</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_High_6_</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Low</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Low_6_</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Close</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Close_6_</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Adj Close</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Adj Close_6_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-31</td>\n",
       "      <td>20.177268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.237734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.302468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.644064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.195705</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.915916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.911882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.921663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.909069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.909069</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-29</td>\n",
       "      <td>23.306868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.054803</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.396112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.645288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.182527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.910118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.915569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.908508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.916207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.916207</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>25.223010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.133909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.305967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.644523</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.362815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.833402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.834459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.834589</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.834453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.834453</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>25.308243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.173425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.041598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.648195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.472294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.691659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.695181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.693237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.700107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.700107</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-05-31</td>\n",
       "      <td>24.875099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.107095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.026945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.645747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.487834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.497700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.494779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.496022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.493870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.493870</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>56.425000</td>\n",
       "      <td>14.590909</td>\n",
       "      <td>1.270279</td>\n",
       "      <td>-0.464069</td>\n",
       "      <td>0.406105</td>\n",
       "      <td>2.629514</td>\n",
       "      <td>2.294724</td>\n",
       "      <td>1.687843</td>\n",
       "      <td>1.435042</td>\n",
       "      <td>...</td>\n",
       "      <td>1.168835</td>\n",
       "      <td>1.083831</td>\n",
       "      <td>1.174279</td>\n",
       "      <td>1.090253</td>\n",
       "      <td>1.163390</td>\n",
       "      <td>1.075285</td>\n",
       "      <td>1.167415</td>\n",
       "      <td>1.087407</td>\n",
       "      <td>1.167415</td>\n",
       "      <td>1.087407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>71.086957</td>\n",
       "      <td>19.083333</td>\n",
       "      <td>2.126927</td>\n",
       "      <td>-0.280700</td>\n",
       "      <td>1.148209</td>\n",
       "      <td>3.299182</td>\n",
       "      <td>2.612162</td>\n",
       "      <td>1.991462</td>\n",
       "      <td>1.241856</td>\n",
       "      <td>...</td>\n",
       "      <td>1.471655</td>\n",
       "      <td>1.124191</td>\n",
       "      <td>1.468981</td>\n",
       "      <td>1.123562</td>\n",
       "      <td>1.476677</td>\n",
       "      <td>1.129411</td>\n",
       "      <td>1.477051</td>\n",
       "      <td>1.129772</td>\n",
       "      <td>1.477051</td>\n",
       "      <td>1.129772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>75.804348</td>\n",
       "      <td>22.750000</td>\n",
       "      <td>2.687810</td>\n",
       "      <td>-0.096964</td>\n",
       "      <td>0.860751</td>\n",
       "      <td>3.926870</td>\n",
       "      <td>2.556884</td>\n",
       "      <td>1.998856</td>\n",
       "      <td>1.425830</td>\n",
       "      <td>...</td>\n",
       "      <td>2.259286</td>\n",
       "      <td>1.020054</td>\n",
       "      <td>2.271036</td>\n",
       "      <td>1.006143</td>\n",
       "      <td>2.255625</td>\n",
       "      <td>1.008416</td>\n",
       "      <td>2.261700</td>\n",
       "      <td>1.008169</td>\n",
       "      <td>2.261700</td>\n",
       "      <td>1.008169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>74.295455</td>\n",
       "      <td>28.170455</td>\n",
       "      <td>2.621695</td>\n",
       "      <td>0.164625</td>\n",
       "      <td>1.713126</td>\n",
       "      <td>1.521353</td>\n",
       "      <td>2.501607</td>\n",
       "      <td>1.794166</td>\n",
       "      <td>1.892086</td>\n",
       "      <td>...</td>\n",
       "      <td>2.106495</td>\n",
       "      <td>1.240984</td>\n",
       "      <td>2.106820</td>\n",
       "      <td>1.248657</td>\n",
       "      <td>2.111163</td>\n",
       "      <td>1.253894</td>\n",
       "      <td>2.116623</td>\n",
       "      <td>1.257281</td>\n",
       "      <td>2.116623</td>\n",
       "      <td>1.257281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>33.043478</td>\n",
       "      <td>2.389233</td>\n",
       "      <td>0.286579</td>\n",
       "      <td>1.060177</td>\n",
       "      <td>1.582281</td>\n",
       "      <td>2.515599</td>\n",
       "      <td>1.839908</td>\n",
       "      <td>2.230336</td>\n",
       "      <td>...</td>\n",
       "      <td>1.628496</td>\n",
       "      <td>1.185124</td>\n",
       "      <td>1.623594</td>\n",
       "      <td>1.187299</td>\n",
       "      <td>1.607981</td>\n",
       "      <td>1.189690</td>\n",
       "      <td>1.608344</td>\n",
       "      <td>1.197156</td>\n",
       "      <td>1.608344</td>\n",
       "      <td>1.197156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 1847 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date          y       y_6_        y2     y2_6_  \\\n",
       "0  2016-01-31  20.177268        NaN -0.237734       NaN   \n",
       "1  2016-02-29  23.306868        NaN -0.054803       NaN   \n",
       "2  2016-03-31  25.223010        NaN  0.133909       NaN   \n",
       "3  2016-04-30  25.308243        NaN  0.173425       NaN   \n",
       "4  2016-05-31  24.875099        NaN  0.107095       NaN   \n",
       "..        ...        ...        ...       ...       ...   \n",
       "73 2022-02-28  56.425000  14.590909  1.270279 -0.464069   \n",
       "74 2022-03-31  71.086957  19.083333  2.126927 -0.280700   \n",
       "75 2022-04-30  75.804348  22.750000  2.687810 -0.096964   \n",
       "76 2022-05-31  74.295455  28.170455  2.621695  0.164625   \n",
       "77 2022-06-30  73.000000  33.043478  2.389233  0.286579   \n",
       "\n",
       "    Baltic Dry Index (BDI)  Baltic Dry Index (BDI)_6_  \\\n",
       "0                -1.302468                        NaN   \n",
       "1                -1.396112                        NaN   \n",
       "2                -1.305967                        NaN   \n",
       "3                -1.041598                        NaN   \n",
       "4                -1.026945                        NaN   \n",
       "..                     ...                        ...   \n",
       "73                0.406105                   2.629514   \n",
       "74                1.148209                   3.299182   \n",
       "75                0.860751                   3.926870   \n",
       "76                1.713126                   1.521353   \n",
       "77                1.060177                   1.582281   \n",
       "\n",
       "    Howe Robinson Container Index  Howe Robinson Container Index_6_  \\\n",
       "0                       -0.644064                               NaN   \n",
       "1                       -0.645288                               NaN   \n",
       "2                       -0.644523                               NaN   \n",
       "3                       -0.648195                               NaN   \n",
       "4                       -0.645747                               NaN   \n",
       "..                            ...                               ...   \n",
       "73                       2.294724                          1.687843   \n",
       "74                       2.612162                          1.991462   \n",
       "75                       2.556884                          1.998856   \n",
       "76                       2.501607                          1.794166   \n",
       "77                       2.515599                          1.839908   \n",
       "\n",
       "    Shanghai Containerized Freight Index (SCFI)  ...  \\\n",
       "0                                     -0.195705  ...   \n",
       "1                                     -0.182527  ...   \n",
       "2                                     -0.362815  ...   \n",
       "3                                     -0.472294  ...   \n",
       "4                                     -0.487834  ...   \n",
       "..                                          ...  ...   \n",
       "73                                     1.435042  ...   \n",
       "74                                     1.241856  ...   \n",
       "75                                     1.425830  ...   \n",
       "76                                     1.892086  ...   \n",
       "77                                     2.230336  ...   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Open  Allkem Limited (AKE.AX)_price_Open_6_  \\\n",
       "0                            -0.915916                                    NaN   \n",
       "1                            -0.910118                                    NaN   \n",
       "2                            -0.833402                                    NaN   \n",
       "3                            -0.691659                                    NaN   \n",
       "4                            -0.497700                                    NaN   \n",
       "..                                 ...                                    ...   \n",
       "73                            1.168835                               1.083831   \n",
       "74                            1.471655                               1.124191   \n",
       "75                            2.259286                               1.020054   \n",
       "76                            2.106495                               1.240984   \n",
       "77                            1.628496                               1.185124   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_High  Allkem Limited (AKE.AX)_price_High_6_  \\\n",
       "0                            -0.911882                                    NaN   \n",
       "1                            -0.915569                                    NaN   \n",
       "2                            -0.834459                                    NaN   \n",
       "3                            -0.695181                                    NaN   \n",
       "4                            -0.494779                                    NaN   \n",
       "..                                 ...                                    ...   \n",
       "73                            1.174279                               1.090253   \n",
       "74                            1.468981                               1.123562   \n",
       "75                            2.271036                               1.006143   \n",
       "76                            2.106820                               1.248657   \n",
       "77                            1.623594                               1.187299   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Low  Allkem Limited (AKE.AX)_price_Low_6_  \\\n",
       "0                           -0.921663                                   NaN   \n",
       "1                           -0.908508                                   NaN   \n",
       "2                           -0.834589                                   NaN   \n",
       "3                           -0.693237                                   NaN   \n",
       "4                           -0.496022                                   NaN   \n",
       "..                                ...                                   ...   \n",
       "73                           1.163390                              1.075285   \n",
       "74                           1.476677                              1.129411   \n",
       "75                           2.255625                              1.008416   \n",
       "76                           2.111163                              1.253894   \n",
       "77                           1.607981                              1.189690   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Close  \\\n",
       "0                             -0.909069   \n",
       "1                             -0.916207   \n",
       "2                             -0.834453   \n",
       "3                             -0.700107   \n",
       "4                             -0.493870   \n",
       "..                                  ...   \n",
       "73                             1.167415   \n",
       "74                             1.477051   \n",
       "75                             2.261700   \n",
       "76                             2.116623   \n",
       "77                             1.608344   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Close_6_  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "..                                     ...   \n",
       "73                                1.087407   \n",
       "74                                1.129772   \n",
       "75                                1.008169   \n",
       "76                                1.257281   \n",
       "77                                1.197156   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Adj Close  \\\n",
       "0                                 -0.909069   \n",
       "1                                 -0.916207   \n",
       "2                                 -0.834453   \n",
       "3                                 -0.700107   \n",
       "4                                 -0.493870   \n",
       "..                                      ...   \n",
       "73                                 1.167415   \n",
       "74                                 1.477051   \n",
       "75                                 2.261700   \n",
       "76                                 2.116623   \n",
       "77                                 1.608344   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Adj Close_6_  \n",
       "0                                          NaN  \n",
       "1                                          NaN  \n",
       "2                                          NaN  \n",
       "3                                          NaN  \n",
       "4                                          NaN  \n",
       "..                                         ...  \n",
       "73                                    1.087407  \n",
       "74                                    1.129772  \n",
       "75                                    1.008169  \n",
       "76                                    1.257281  \n",
       "77                                    1.197156  \n",
       "\n",
       "[78 rows x 1847 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_y_set_shifted_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637d45e9-c3d2-4965-af8f-5553849d1f41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_y_set_shifted_merged_corr = raw_y_set_shifted_merged.corr()\n",
    "raw_y_set_shifted_merged_filtered = raw_y_set_shifted_merged_corr[raw_y_set_resampled.columns.drop('date')]\n",
    "# raw_y_set_shifted_merged_filtered = raw_y_set_shifted_merged_filtered.loc[raw_y_set_shifted_merged_filtered.index != 'y']\n",
    "# raw_y_set_shifted_merged_filtered = raw_y_set_shifted_merged_filtered.loc[raw_y_set_shifted_merged_filtered.index != 'y2']\n",
    "raw_y_set_shifted_merged_filtered_y = raw_y_set_shifted_merged_filtered.iloc[:,:1]\n",
    "raw_y_set_shifted_merged_filtered_upto_corr_rate = raw_y_set_shifted_merged_filtered_y[raw_y_set_shifted_merged_filtered_y.y >= corr_rate]\n",
    "raw_y_set_shifted_merged_filtered_upto_corr_rate = raw_y_set_shifted_merged_filtered_upto_corr_rate[raw_y_set_shifted_merged_filtered_upto_corr_rate.index.str.contains(f'_{test_period}_')]\n",
    "raw_y_set_shifted_merged_filtered_upto_corr_rate = raw_y_set_shifted_merged_filtered_upto_corr_rate[:future_variable_limit]\n",
    "\n",
    "futr_list = raw_y_set_shifted_merged_filtered_upto_corr_rate.index.values.tolist()\n",
    "futr_list = [word.replace(f\"_{test_period}_\", \"\") for word in futr_list]\n",
    "\n",
    "# if 'li2co395_am_kg' in futr_list:\n",
    "#     futr_list = [w.replace('li2co395_am_kg', 'li2co3_995_am_kg') for w in futr_list]\n",
    "# raw_y_set_shifted_merged_filtered_upto_corr_rate = raw_y_set_shifted_merged_filtered_upto_corr_rate.sort_values('y',ascending=False)\n",
    "\n",
    "# if 'y2' in futr_list:\n",
    "#     (futr_list).remove('y2')\n",
    "# if 'li2co395_am_kg' in futr_list:\n",
    "#     (futr_list).remove('li2co395_am_kg')\n",
    "# if 'li2co3_995_am_kg' in futr_list:\n",
    "#     (futr_list).remove('li2co3_995_am_kg')\n",
    "# if 'mn_mb_kg' in futr_list:\n",
    "#     (futr_list).remove('mn_mb_kg')\n",
    "# if 'al_lme_kg' in futr_list:\n",
    "#     (futr_list).remove('al_lme_kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27019fc5-510b-4bd6-97ce-001677901aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baltic Dry Index (BDI)_6_</th>\n",
       "      <td>0.745130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Howe Robinson Container Index_6_</th>\n",
       "      <td>0.913334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         y\n",
       "Baltic Dry Index (BDI)_6_         0.745130\n",
       "Howe Robinson Container Index_6_  0.913334"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_y_set_shifted_merged_filtered_upto_corr_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6792eab-6024-4482-8075-4895bd41586c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Baltic Dry Index (BDI)', 'Howe Robinson Container Index']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "futr_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e2423-991f-4d6b-882c-50617a2a65e1",
   "metadata": {},
   "source": [
    "#### 해당 기간에서의 corr best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df77e790-0ad9-4403-abdd-53ebc8ad6996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_variable_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e40279d3-f9f2-410e-88ac-452b91a2f75c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPI_PRT_TOT_FOODENRG_AGRWTH',\n",
       " '대두 [미국(생산자 가격)] 현물',\n",
       " 'CPI_BEL_TOT_AGRWTH',\n",
       " 'CPI_PRT_TOT_AGRWTH',\n",
       " '중국 니켈 SEMIS 수입량']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_var_chcek = raw_y_set_resampled.iloc[:,1:]\n",
    "\n",
    "hist_var_corr = hist_var_chcek.corr()\n",
    "hist_var_corr_filtered = hist_var_corr.loc[hist_var_corr.index != 'y']\n",
    "hist_var_corr_filtered = hist_var_corr_filtered.loc[hist_var_corr_filtered.index != 'y2']\n",
    "hist_var_corr_filtered_y = hist_var_corr_filtered.iloc[:,:1]\n",
    "hist_var_corr_filtered_y_corr_rate = hist_var_corr_filtered_y[hist_var_corr_filtered_y.y >= hist_corr_rate]\n",
    "hist_var_corr_filtered_y_only = hist_var_corr_filtered_y_corr_rate[hist_var_corr_filtered_y_corr_rate.index.str.contains('li2co3')==False]\n",
    "hist_var_corr_filtered_y_only = hist_var_corr_filtered_y_only[hist_var_corr_filtered_y_only.index.str.contains('lioh')==False]\n",
    "hist_var_corr_filtered_y_only = hist_var_corr_filtered_y_only.sort_values('y',ascending=False)\n",
    "hist_var_list = hist_var_corr_filtered_y_only.index.values.tolist()\n",
    "\n",
    "hist_list = hist_var_list[:hist_variable_limit]\n",
    "hist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c2422e-32ee-42b3-abb8-c7fde1f56a45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CPI_PRT_TOT_FOODENRG_AGRWTH</th>\n",
       "      <td>0.947163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>대두 [미국(생산자 가격)] 현물</th>\n",
       "      <td>0.946299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPI_BEL_TOT_AGRWTH</th>\n",
       "      <td>0.938752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPI_PRT_TOT_AGRWTH</th>\n",
       "      <td>0.934432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>중국 니켈 SEMIS 수입량</th>\n",
       "      <td>0.903346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPI_GBR_TOT_FOODENRG_AGRWTH</th>\n",
       "      <td>0.902181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPI_LTU_TOT_AGRWTH</th>\n",
       "      <td>0.901834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPI_GRC_TOT_AGRWTH</th>\n",
       "      <td>0.900623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    y\n",
       "CPI_PRT_TOT_FOODENRG_AGRWTH  0.947163\n",
       "대두 [미국(생산자 가격)] 현물           0.946299\n",
       "CPI_BEL_TOT_AGRWTH           0.938752\n",
       "CPI_PRT_TOT_AGRWTH           0.934432\n",
       "중국 니켈 SEMIS 수입량              0.903346\n",
       "CPI_GBR_TOT_FOODENRG_AGRWTH  0.902181\n",
       "CPI_LTU_TOT_AGRWTH           0.901834\n",
       "CPI_GRC_TOT_AGRWTH           0.900623"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_var_corr_filtered_y_only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a209346-8f24-44ee-ba3b-a08f80614190",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### static DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ce7033a-2234-4bac-9087-91daee3af55a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "static_df = pd.read_csv('/users/ESKO/PYWORK/NHITS/data/static_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fba8f2-a5ab-484b-b1ad-bda6ea55f7af",
   "metadata": {},
   "source": [
    "## <Training & Test>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135deb5-4427-45d2-b085-d0063d554f54",
   "metadata": {},
   "source": [
    "#### 날짜 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "525afb1e-6ec3-4108-9c23-95cc5565c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df.loc[df.ds < '2021-06-30']\n",
    "# test = df.loc[df.ds > '2021-06-30']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95faab-68f4-4566-9494-35d2b646a7d7",
   "metadata": {},
   "source": [
    "#### 날짜 별 len 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8548a462-acb8-462e-afcb-94c8c29af9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(df.loc[df.ds<'2021-06-30'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "837791bc-7eab-4e22-9501-5dda050fa80b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(Y_train_df)\n",
    "# len(Y_test_df)\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48e6ed-d091-4f7e-860d-32db64006c26",
   "metadata": {},
   "source": [
    "Test 용 DF 및 선행 변수 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7947178-5017-4afa-99bb-de9843ac3061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_train_df = df[df.ds<=cutoff_date]\n",
    "Y_test_df = df[df.ds>cutoff_date]\n",
    "\n",
    "if len(Y_test_df) > 12:    \n",
    "    Y_test_df = Y_test_df[:12]\n",
    "else:\n",
    "    pass\n",
    "# Y_train_df = df[df.ds<df['ds'].values[len(df.loc[df.ds<=cutoff_date])]]\n",
    "# Y_test_df = df[df.ds>=df['ds'].values[len(df.loc[df.ds<=cutoff_date])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfdd51ae-12ad-4971-b823-3b24a0321736",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Baltic Dry Index (BDI)', 'Howe Robinson Container Index']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "futr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6a349e3-e146-47f1-aae9-fd4ca29154e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>Baltic Dry Index (BDI)</th>\n",
       "      <th>Howe Robinson Container Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>A</td>\n",
       "      <td>0.318707</td>\n",
       "      <td>1.983558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>A</td>\n",
       "      <td>0.406105</td>\n",
       "      <td>2.294724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>A</td>\n",
       "      <td>1.148209</td>\n",
       "      <td>2.612162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-31</td>\n",
       "      <td>A</td>\n",
       "      <td>0.860751</td>\n",
       "      <td>2.556884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>A</td>\n",
       "      <td>1.713126</td>\n",
       "      <td>2.501607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>A</td>\n",
       "      <td>1.060177</td>\n",
       "      <td>2.515599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ds unique_id  Baltic Dry Index (BDI)  Howe Robinson Container Index\n",
       "0 2022-07-31         A                0.318707                       1.983558\n",
       "1 2022-08-31         A                0.406105                       2.294724\n",
       "2 2022-09-30         A                1.148209                       2.612162\n",
       "3 2022-10-31         A                0.860751                       2.556884\n",
       "4 2022-11-30         A                1.713126                       2.501607\n",
       "5 2022-12-31         A                1.060177                       2.515599"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_date_series = pd.date_range(pd.date_range(cutoff_date,periods=2,freq='M')[1],periods=test_period,freq='M')\n",
    "future_date = pd.DataFrame(future_date_series,columns=['ds'])\n",
    "# future_date['unique_id'] = 'A'\n",
    "future_df_temp = pd.concat([raw_y_set_resampled.rename(columns={'date':'ds'}).iloc[-(test_period+len(Y_test_df)):,:],future_date])\n",
    "future_df_temp[futr_list] = future_df_temp[futr_list].shift(test_period)\n",
    "\n",
    "future_df_with_index = list(['ds','unique_id']) + futr_list\n",
    "future_df_with_index\n",
    "\n",
    "future_df_temp = future_df_temp[future_df_temp.columns[future_df_temp.columns.isin(future_df_with_index)]].reset_index(drop=True)\n",
    "future_df_temp.insert(1, 'unique_id', 'A')\n",
    "futr_df = future_df_temp.iloc[-test_period:,:].reset_index(drop=True)\n",
    "futr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b52225-6b2d-4ffb-939e-beab3e28281e",
   "metadata": {},
   "source": [
    "#### No 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e57c2c7e-3a33-4965-adec-88ef864f2038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# horizon = len(Y_test_df)\n",
    "\n",
    "# models = [\n",
    "#           # NBEATS(input_size=5 * horizon, gpus = 1, h=horizon, max_epochs=100),\n",
    "#           # NBEATSx(h=12, input_size=24,\n",
    "#           #       # loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "#           #       gpus=1,\n",
    "#           #       scaler_type='robust',\n",
    "#           #       stat_exog_list = None,  \n",
    "#           #       futr_exog_list = futr_list,\n",
    "#           #       max_steps=100,\n",
    "#           #       # val_check_steps=10,\n",
    "#           #       # early_stop_patience_steps=2\n",
    "#           #      ),\n",
    "#           NHITS(input_size=5 * horizon, \n",
    "#                 h=horizon+1,\n",
    "#                 gpus = 1,\n",
    "#                 stat_exog_list = None,\n",
    "#                 hist_exog_list=None,\n",
    "#                 futr_exog_list = None,\n",
    "#                 # futr_exog_list = futr_list,\n",
    "#                 # n_freq_downsample=[24, 12, 1],\n",
    "#                 # mlp_units = [[512, 512], [512, 512], [512, 512]],\n",
    "#                 # n_pool_kernel_size = [2, 2, 1],\n",
    "#                 # n_freq_downsample=[24, 12, 1],\n",
    "#                 scaler_type = 'standard',\n",
    "#                 learning_rate=1e-4,\n",
    "#                 pooling_mode = 'MaxPool1d',\n",
    "#                 activation='ReLU',\n",
    "#                 batch_size=128,\n",
    "#                 random_seed=42,\n",
    "#                 max_epochs=200\n",
    "#                ),\n",
    "#            # NHITS(\n",
    "#            #       h,\n",
    "#            #       input_size,\n",
    "#            #       futr_exog_list=None,\n",
    "#            #       hist_exog_list=None,\n",
    "#            #       stat_exog_list=None,\n",
    "#            #       stack_types: list = ['identity', 'identity', 'identity'],\n",
    "#            #       n_blocks: list = [1, 1, 1],\n",
    "#            #       mlp_units: list = [[512, 512], [512, 512], [512, 512]],\n",
    "#            #       n_pool_kernel_size: list = [2, 2, 1],\n",
    "#            #       n_freq_downsample: list = [4, 2, 1],\n",
    "#            #       pooling_mode: str = 'MaxPool1d',\n",
    "#            #       interpolation_mode: str = 'linear',\n",
    "#            #       dropout_prob_theta=0.0,\n",
    "#            #       activation='ReLU',\n",
    "#            #       loss=MAE(),\n",
    "#            #       learning_rate=0.001,\n",
    "#            #       batch_size=32,\n",
    "#            #       windows_batch_size: int = 1024,\n",
    "#            #       step_size: int = 1,\n",
    "#            #       scaler_type='identity',\n",
    "#            #       random_seed=1,\n",
    "#            #       num_workers_loader=0,\n",
    "#            #       drop_last_loader=False,\n",
    "#            #       **trainer_kwargs,\n",
    "    \n",
    "#           # TFT(h=12, \n",
    "#           #     input_size=5 * horizon,\n",
    "#           #     # hidden_size=20,\n",
    "#           #     #loss=DistributionLoss(distribution='Poisson', level=[80, 90]),\n",
    "#           #     #loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "#           #     #loss=DistributionLoss(distribution='StudentT', level=[80, 90]),\n",
    "#           #     learning_rate=0.005,\n",
    "#           #     stat_exog_list = None,\n",
    "#           #     futr_exog_list = futr_list,\n",
    "#           #     max_steps=100,\n",
    "#           #     gpus=1,\n",
    "#           #     # val_check_steps=10,\n",
    "#           #     # early_stop_patience_steps=10,\n",
    "#           #     scaler_type='robust',\n",
    "#           #     # windows_batch_size=None,\n",
    "#           #     enable_progress_bar=True),\n",
    "#              ]\n",
    "# nforecast_nv = NeuralForecast(models=models, freq='M')\n",
    "# nforecast_nv.fit(df=Y_train_df)\n",
    "# Y_hat_df_nv = nforecast_nv.predict(futr_df=futr_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23a1910f-285e-457a-888e-8ca4fa041ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot predictions\n",
    "# fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "# Y_hat_df_nv = Y_test_df.merge(Y_hat_df_nv, how='left', on=['unique_id', 'ds'])\n",
    "# plot_df_nv = pd.concat([Y_train_df, Y_hat_df_nv]).set_index('ds')\n",
    "\n",
    "# plot_df_nv[['y', 'NHITS']].plot(ax=ax, linewidth=2, marker='o')\n",
    "# plt.axvline(cutoff_date, color='red')\n",
    "\n",
    "# ax.set_title(f'LI2CO3 Actual and Predicted Plot - Cutoff: {cutoff_date}', fontsize=22)\n",
    "# ax.set_ylabel('LI2CO3_PRICE', fontsize=20)\n",
    "# ax.set_xlabel('Date', fontsize=20)\n",
    "# ax.legend(prop={'size': 15})\n",
    "# ax.grid()\n",
    "\n",
    "# fig.savefig(f'./data/forecast_plot_{cutoff_date}_nv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "940332ca-281c-41c8-ac8f-2bc9646f7638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_df_nv[['y', 'NHITS']].to_csv(f'./data/forecasting_{cutoff_date}_nv.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6e49b-657e-48e0-b389-d96d77b28470",
   "metadata": {},
   "source": [
    "#### Yes 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d139885f-94df-4437-9a56-412123466be7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>y</th>\n",
       "      <th>lioh_cif_fast_kg_spot_exchng</th>\n",
       "      <th>Baltic Dry Index (BDI)</th>\n",
       "      <th>Howe Robinson Container Index</th>\n",
       "      <th>Shanghai Containerized Freight Index (SCFI)</th>\n",
       "      <th>옥수수 For Grain [미국(생산자 가격)] 현물</th>\n",
       "      <th>원당 (daily) [ISA] 현물</th>\n",
       "      <th>대두 [미국(생산자 가격)] 현물</th>\n",
       "      <th>...</th>\n",
       "      <th>Pilbara Mineral Limited_price_Open</th>\n",
       "      <th>Pilbara Mineral Limited_price_High</th>\n",
       "      <th>Pilbara Mineral Limited_price_Low</th>\n",
       "      <th>Pilbara Mineral Limited_price_Close</th>\n",
       "      <th>Pilbara Mineral Limited_price_Adj Close</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Open</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_High</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Low</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Close</th>\n",
       "      <th>Allkem Limited (AKE.AX)_price_Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>A</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>2.389233</td>\n",
       "      <td>0.691597</td>\n",
       "      <td>2.439292</td>\n",
       "      <td>2.526593</td>\n",
       "      <td>2.377100</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>2.852368</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464141</td>\n",
       "      <td>1.460176</td>\n",
       "      <td>1.481963</td>\n",
       "      <td>1.473009</td>\n",
       "      <td>2.410643</td>\n",
       "      <td>1.436494</td>\n",
       "      <td>1.434052</td>\n",
       "      <td>1.439497</td>\n",
       "      <td>1.441548</td>\n",
       "      <td>1.441548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>A</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>2.420773</td>\n",
       "      <td>-0.092744</td>\n",
       "      <td>2.119559</td>\n",
       "      <td>2.773368</td>\n",
       "      <td>2.369193</td>\n",
       "      <td>0.793818</td>\n",
       "      <td>2.593314</td>\n",
       "      <td>...</td>\n",
       "      <td>2.219001</td>\n",
       "      <td>2.233095</td>\n",
       "      <td>2.245839</td>\n",
       "      <td>2.264053</td>\n",
       "      <td>2.410643</td>\n",
       "      <td>2.144853</td>\n",
       "      <td>2.155770</td>\n",
       "      <td>2.181382</td>\n",
       "      <td>2.184215</td>\n",
       "      <td>2.184215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>A</td>\n",
       "      <td>74.047619</td>\n",
       "      <td>2.507937</td>\n",
       "      <td>-0.004062</td>\n",
       "      <td>1.188816</td>\n",
       "      <td>3.024986</td>\n",
       "      <td>2.250597</td>\n",
       "      <td>0.750690</td>\n",
       "      <td>1.621862</td>\n",
       "      <td>...</td>\n",
       "      <td>2.107414</td>\n",
       "      <td>2.092161</td>\n",
       "      <td>2.084336</td>\n",
       "      <td>2.084871</td>\n",
       "      <td>1.970948</td>\n",
       "      <td>2.828439</td>\n",
       "      <td>2.823884</td>\n",
       "      <td>2.834908</td>\n",
       "      <td>2.836251</td>\n",
       "      <td>2.836251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2022-10-31</td>\n",
       "      <td>A</td>\n",
       "      <td>77.673913</td>\n",
       "      <td>2.664680</td>\n",
       "      <td>0.381286</td>\n",
       "      <td>0.204375</td>\n",
       "      <td>3.036207</td>\n",
       "      <td>1.784119</td>\n",
       "      <td>0.691518</td>\n",
       "      <td>1.427572</td>\n",
       "      <td>...</td>\n",
       "      <td>2.352264</td>\n",
       "      <td>2.359441</td>\n",
       "      <td>2.354164</td>\n",
       "      <td>2.362142</td>\n",
       "      <td>2.222022</td>\n",
       "      <td>2.765597</td>\n",
       "      <td>2.782167</td>\n",
       "      <td>2.764528</td>\n",
       "      <td>2.761120</td>\n",
       "      <td>2.761120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>A</td>\n",
       "      <td>80.818182</td>\n",
       "      <td>2.828879</td>\n",
       "      <td>-0.226024</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>3.004479</td>\n",
       "      <td>1.784119</td>\n",
       "      <td>1.005474</td>\n",
       "      <td>1.621862</td>\n",
       "      <td>...</td>\n",
       "      <td>2.433279</td>\n",
       "      <td>2.443712</td>\n",
       "      <td>2.455263</td>\n",
       "      <td>2.452607</td>\n",
       "      <td>2.303940</td>\n",
       "      <td>2.830058</td>\n",
       "      <td>2.817705</td>\n",
       "      <td>2.816214</td>\n",
       "      <td>2.795982</td>\n",
       "      <td>2.795982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>A</td>\n",
       "      <td>80.882353</td>\n",
       "      <td>2.867154</td>\n",
       "      <td>-0.043847</td>\n",
       "      <td>-0.041773</td>\n",
       "      <td>3.301048</td>\n",
       "      <td>1.784119</td>\n",
       "      <td>1.131385</td>\n",
       "      <td>1.816153</td>\n",
       "      <td>...</td>\n",
       "      <td>1.980999</td>\n",
       "      <td>1.967237</td>\n",
       "      <td>1.981728</td>\n",
       "      <td>1.959693</td>\n",
       "      <td>1.857597</td>\n",
       "      <td>2.230517</td>\n",
       "      <td>2.206089</td>\n",
       "      <td>2.211335</td>\n",
       "      <td>2.207568</td>\n",
       "      <td>2.207568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 925 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds unique_id          y  lioh_cif_fast_kg_spot_exchng  \\\n",
       "78 2022-07-31         A  73.000000                      2.389233   \n",
       "79 2022-08-31         A  73.000000                      2.420773   \n",
       "80 2022-09-30         A  74.047619                      2.507937   \n",
       "81 2022-10-31         A  77.673913                      2.664680   \n",
       "82 2022-11-30         A  80.818182                      2.828879   \n",
       "83 2022-12-31         A  80.882353                      2.867154   \n",
       "\n",
       "    Baltic Dry Index (BDI)  Howe Robinson Container Index  \\\n",
       "78                0.691597                       2.439292   \n",
       "79               -0.092744                       2.119559   \n",
       "80               -0.004062                       1.188816   \n",
       "81                0.381286                       0.204375   \n",
       "82               -0.226024                      -0.008117   \n",
       "83               -0.043847                      -0.041773   \n",
       "\n",
       "    Shanghai Containerized Freight Index (SCFI)  \\\n",
       "78                                     2.526593   \n",
       "79                                     2.773368   \n",
       "80                                     3.024986   \n",
       "81                                     3.036207   \n",
       "82                                     3.004479   \n",
       "83                                     3.301048   \n",
       "\n",
       "    옥수수 For Grain [미국(생산자 가격)] 현물  원당 (daily) [ISA] 현물  대두 [미국(생산자 가격)] 현물  \\\n",
       "78                       2.377100             0.921668            2.852368   \n",
       "79                       2.369193             0.793818            2.593314   \n",
       "80                       2.250597             0.750690            1.621862   \n",
       "81                       1.784119             0.691518            1.427572   \n",
       "82                       1.784119             1.005474            1.621862   \n",
       "83                       1.784119             1.131385            1.816153   \n",
       "\n",
       "    ...  Pilbara Mineral Limited_price_Open  \\\n",
       "78  ...                            1.464141   \n",
       "79  ...                            2.219001   \n",
       "80  ...                            2.107414   \n",
       "81  ...                            2.352264   \n",
       "82  ...                            2.433279   \n",
       "83  ...                            1.980999   \n",
       "\n",
       "    Pilbara Mineral Limited_price_High  Pilbara Mineral Limited_price_Low  \\\n",
       "78                            1.460176                           1.481963   \n",
       "79                            2.233095                           2.245839   \n",
       "80                            2.092161                           2.084336   \n",
       "81                            2.359441                           2.354164   \n",
       "82                            2.443712                           2.455263   \n",
       "83                            1.967237                           1.981728   \n",
       "\n",
       "    Pilbara Mineral Limited_price_Close  \\\n",
       "78                             1.473009   \n",
       "79                             2.264053   \n",
       "80                             2.084871   \n",
       "81                             2.362142   \n",
       "82                             2.452607   \n",
       "83                             1.959693   \n",
       "\n",
       "    Pilbara Mineral Limited_price_Adj Close  \\\n",
       "78                                 2.410643   \n",
       "79                                 2.410643   \n",
       "80                                 1.970948   \n",
       "81                                 2.222022   \n",
       "82                                 2.303940   \n",
       "83                                 1.857597   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Open  Allkem Limited (AKE.AX)_price_High  \\\n",
       "78                            1.436494                            1.434052   \n",
       "79                            2.144853                            2.155770   \n",
       "80                            2.828439                            2.823884   \n",
       "81                            2.765597                            2.782167   \n",
       "82                            2.830058                            2.817705   \n",
       "83                            2.230517                            2.206089   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Low  Allkem Limited (AKE.AX)_price_Close  \\\n",
       "78                           1.439497                             1.441548   \n",
       "79                           2.181382                             2.184215   \n",
       "80                           2.834908                             2.836251   \n",
       "81                           2.764528                             2.761120   \n",
       "82                           2.816214                             2.795982   \n",
       "83                           2.211335                             2.207568   \n",
       "\n",
       "    Allkem Limited (AKE.AX)_price_Adj Close  \n",
       "78                                 1.441548  \n",
       "79                                 2.184215  \n",
       "80                                 2.836251  \n",
       "81                                 2.761120  \n",
       "82                                 2.795982  \n",
       "83                                 2.207568  \n",
       "\n",
       "[6 rows x 925 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16b3c015-bcd6-48b4-a847-d414d3e093e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dd3e1a01fe45d4ac40ab7f30630398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81aac1c739a641c3943537c3e0c5681a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 6.00 GiB total capacity; 447.88 MiB already allocated; 4.08 GiB free; 552.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 113\u001b[0m\n\u001b[0;32m     12\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m           \u001b[38;5;66;03m# NBEATS(input_size=5 * horizon, gpus = 1, h=horizon, max_epochs=100),\u001b[39;00m\n\u001b[0;32m     14\u001b[0m           NBEATSx(h\u001b[38;5;241m=\u001b[39mhorizon, input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m horizon,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m           \u001b[38;5;66;03m#    ),\u001b[39;00m\n\u001b[0;32m    111\u001b[0m              ]\n\u001b[0;32m    112\u001b[0m nforecast \u001b[38;5;241m=\u001b[39m NeuralForecast(models\u001b[38;5;241m=\u001b[39mmodels, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m \u001b[43mnforecast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m Y_hat_df \u001b[38;5;241m=\u001b[39m nforecast\u001b[38;5;241m.\u001b[39mpredict(futr_df\u001b[38;5;241m=\u001b[39mfutr_df)\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\neuralforecast\\core.py:157\u001b[0m, in \u001b[0;36mNeuralForecast.fit\u001b[1;34m(self, df, static_df, val_size, sort_df, verbose)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# train + validation\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[1;32m--> 157\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# train with the full dataset\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\neuralforecast\\common\\_base_windows.py:493\u001b[0m, in \u001b[0;36mBaseWindows.fit\u001b[1;34m(self, dataset, val_size, test_size)\u001b[0m\n\u001b[0;32m    485\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m TimeSeriesDataModule(\n\u001b[0;32m    486\u001b[0m     dataset,\n\u001b[0;32m    487\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    488\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers_loader,\n\u001b[0;32m    489\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_last_loader,\n\u001b[0;32m    490\u001b[0m )\n\u001b[0;32m    492\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer_kwargs)\n\u001b[1;32m--> 493\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 770\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 723\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    807\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    809\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    810\u001b[0m )\n\u001b[1;32m--> 811\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m-> 1236\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1238\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1353\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:266\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(\n\u001b[0;32m    263\u001b[0m     dataloader, batch_to_device\u001b[38;5;241m=\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_strategy_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataloader_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    264\u001b[0m )\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:208\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 208\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m     87\u001b[0m     optimizers \u001b[38;5;241m=\u001b[39m _get_active_optimizers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[1;32m---> 88\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_loop\u001b[38;5;241m.\u001b[39mrun(split_batch, batch_idx)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:203\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[1;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madvance\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_optimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim_progress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_position\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;66;03m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;66;03m# would be skipped otherwise\u001b[39;00m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_idx] \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39masdict()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:256\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[1;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[0;32m    249\u001b[0m         closure()\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:369\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[1;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_tpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTPUAccelerator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_native_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mAMPType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNATIVE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_lbfgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_lbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1595\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1592\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1595\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py:1646\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1566\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1573\u001b[0m     using_lbfgs: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;124;03m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m \u001b[38;5;124;03m    each optimizer.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1644\u001b[0m \n\u001b[0;32m   1645\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1646\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:168\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:193\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual optimizer step.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    **kwargs: Any extra arguments to ``optimizer.step``\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:155\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m    154\u001b[0m     closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\optim\\adam.py:183\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 183\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    186\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:140\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    129\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:148\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:134\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 134\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:427\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[1;34m(self, split_batch, batch_idx, opt_idx)\u001b[0m\n\u001b[0;32m    422\u001b[0m step_kwargs \u001b[38;5;241m=\u001b[39m _build_training_step_kwargs(\n\u001b[0;32m    423\u001b[0m     lightning_module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers, split_batch, batch_idx, opt_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hiddens\n\u001b[0;32m    424\u001b[0m )\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 427\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[0;32m    430\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, training_step_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1765\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1765\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1768\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:333\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The actual training step.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.training_step` for more details\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\neuralforecast\\common\\_base_windows.py:318\u001b[0m, in \u001b[0;36mBaseWindows.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# Create and normalize windows [Ws, L+H, C]\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_windows(batch, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 318\u001b[0m     windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# Parse windows\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     (\n\u001b[0;32m    322\u001b[0m         insample_y,\n\u001b[0;32m    323\u001b[0m         insample_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m         stat_exog,\n\u001b[0;32m    329\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_windows(batch, windows)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\neuralforecast\\common\\_base_windows.py:241\u001b[0m, in \u001b[0;36mBaseWindows._normalization\u001b[1;34m(self, windows)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Normalize. self.scaler stores the shift and scale for inverse transform\u001b[39;00m\n\u001b[0;32m    238\u001b[0m temporal_mask \u001b[38;5;241m=\u001b[39m temporal_mask\u001b[38;5;241m.\u001b[39munsqueeze(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    240\u001b[0m )  \u001b[38;5;66;03m# Add channel dimension for scaler.transform.\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m temporal_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemporal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemporal_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Replace values in windows dict\u001b[39;00m\n\u001b[0;32m    244\u001b[0m temporal[:, :, temporal_cols\u001b[38;5;241m.\u001b[39mget_indexer(temporal_data_cols)] \u001b[38;5;241m=\u001b[39m temporal_data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\neuralforecast\\common\\_scalers.py:348\u001b[0m, in \u001b[0;36mTemporalNorm.transform\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Center and scale the data.\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    **Parameters:**<br>\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m    `z`: torch.Tensor same shape as `x`, except scaled.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m     z, x_shift, x_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_shift \u001b[38;5;241m=\u001b[39m x_shift\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_scale \u001b[38;5;241m=\u001b[39m x_scale\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\neuralforecast\\common\\_scalers.py:202\u001b[0m, in \u001b[0;36mrobust_scaler\u001b[1;34m(x, mask, dim, eps)\u001b[0m\n\u001b[0;32m    199\u001b[0m x_mad \u001b[38;5;241m=\u001b[39m masked_median(x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mabs(x \u001b[38;5;241m-\u001b[39m x_median), mask\u001b[38;5;241m=\u001b[39mmask, dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Protect x_mad=0 values\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m x_means \u001b[38;5;241m=\u001b[39m \u001b[43mmasked_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m x_stds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(masked_mean(x\u001b[38;5;241m=\u001b[39m(x \u001b[38;5;241m-\u001b[39m x_means) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, mask\u001b[38;5;241m=\u001b[39mmask, dim\u001b[38;5;241m=\u001b[39mdim))\n\u001b[0;32m    204\u001b[0m x_mad_aux \u001b[38;5;241m=\u001b[39m x_stds \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m0.6744897501960817\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\neuralforecast\\common\\_scalers.py:52\u001b[0m, in \u001b[0;36mmasked_mean\u001b[1;34m(x, mask, dim, keepdim)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Masked  Mean\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03mCompute the mean of tensor `x` along dimension, ignoring values where\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m`x_mean`: torch.Tensor with normalized values.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m x_nan \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 52\u001b[0m x_mean \u001b[38;5;241m=\u001b[39m \u001b[43mx_nan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_mean\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 6.00 GiB total capacity; 447.88 MiB already allocated; 4.08 GiB free; 552.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "if len(Y_test_df) > 0:\n",
    "    horizon = len(Y_test_df)\n",
    "else:\n",
    "    horizon = len(futr_df)\n",
    "    \n",
    "    \n",
    "if cutoff_date <= '2021-12-31':\n",
    "    flexible = None\n",
    "else:\n",
    "    flexible = futr_df\n",
    "\n",
    "models = [\n",
    "          # NBEATS(input_size=5 * horizon, gpus = 1, h=horizon, max_epochs=100),\n",
    "          NBEATSx(h=horizon, input_size=5 * horizon,\n",
    "                # loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "                gpus=1,\n",
    "                scaler_type='robust',\n",
    "                stat_exog_list = None,\n",
    "                # hist_exog_list = hist_list,\n",
    "                # futr_exog_list = None,  \n",
    "                futr_exog_list = flexible,\n",
    "                learning_rate=1e-4,\n",
    "                mlp_units = [[512, 512], [512, 512], [512, 512]],\n",
    "                batch_size=512,\n",
    "                random_seed=42,\n",
    "                n_blocks=[1, 1, 1],\n",
    "                # val_check_steps=10,\n",
    "                # early_stop_patience_steps=2\n",
    "                max_steps=10\n",
    "               ),\n",
    "          NHITS(input_size=5 * horizon, \n",
    "                h=horizon,\n",
    "                gpus = 1,\n",
    "                stat_exog_list = None,\n",
    "                # futr_exog_list = None,\n",
    "                # hist_exog_list = hist_list,\n",
    "                futr_exog_list = flexible,\n",
    "                n_blocks = [1, 1, 1],\n",
    "                mlp_units = [[512, 512], [512, 512], [512, 512]],\n",
    "                n_pool_kernel_size = [2, 2, 1],\n",
    "                n_freq_downsample=[24, 12, 1],\n",
    "                scaler_type = 'robust',\n",
    "                learning_rate=1e-4,\n",
    "                pooling_mode = 'MaxPool1d',\n",
    "                activation='ReLU',\n",
    "                batch_size=512,\n",
    "                random_seed=42,\n",
    "                max_epochs=10\n",
    "               ),\n",
    "          # LSTM(h=horizon, input_size=5 * horizon,\n",
    "          #        # loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "          #        scaler_type='robust',\n",
    "          #        encoder_n_layers=2,\n",
    "          #        encoder_hidden_size=128,\n",
    "          #        context_size=10,\n",
    "          #        decoder_hidden_size=128,\n",
    "          #        decoder_layers=2,\n",
    "          #        gpus = 1,\n",
    "          #        futr_exog_list=futr_df,\n",
    "          #        hist_exog_list=hist_list,\n",
    "          #        stat_exog_list=None,\n",
    "          #        max_steps=10\n",
    "          #        ),\n",
    "           # NHITS(\n",
    "           #       h,\n",
    "           #       input_size,\n",
    "           #       futr_exog_list=None,\n",
    "           #       hist_exog_list=None,\n",
    "           #       stat_exog_list=None,\n",
    "           #       stack_types: list = ['identity', 'identity', 'identity'],\n",
    "           #       n_blocks: list = [1, 1, 1],\n",
    "           #       mlp_units: list = [[512, 512], [512, 512], [512, 512]],\n",
    "           #       n_pool_kernel_size: list = [2, 2, 1],\n",
    "           #       n_freq_downsample: list = [4, 2, 1],\n",
    "           #       pooling_mode: str = 'MaxPool1d',\n",
    "           #       interpolation_mode: str = 'linear',\n",
    "           #       dropout_prob_theta=0.0,\n",
    "           #       activation='ReLU',\n",
    "           #       loss=MAE(),\n",
    "           #       learning_rate=0.001,\n",
    "           #       batch_size=32,\n",
    "           #       windows_batch_size: int = 1024,\n",
    "           #       step_size: int = 1,\n",
    "           #       scaler_type='identity',\n",
    "           #       random_seed=1,\n",
    "           #       num_workers_loader=0,\n",
    "           #       drop_last_loader=False,\n",
    "           #       **trainer_kwargs,\n",
    "    \n",
    "          # TFT(h=horizon, \n",
    "          #     input_size=5 * horizon,\n",
    "          #     #loss=DistributionLoss(distribution='Poisson', level=[80, 90]),\n",
    "          #     #loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "          #     #loss=DistributionLoss(distribution='StudentT', level=[80, 90]),\n",
    "          #     # hidden_size=128,\n",
    "          #     # n_head=4, \n",
    "          #     # attn_dropout=0.0,\n",
    "          #     hist_exog_list=hist_list,\n",
    "          #     dropout=0.1,\n",
    "          #     learning_rate=0.005,\n",
    "          #     stat_exog_list = None,\n",
    "          #     futr_exog_list = flexible,\n",
    "          #     gpus=1,\n",
    "          #     # val_check_steps=10,\n",
    "          #     # early_stop_patience_steps=10,\n",
    "          #     scaler_type='robust',\n",
    "          #     # windows_batch_size=None,\n",
    "          #     enable_progress_bar=True,\n",
    "          #     max_steps=10\n",
    "          #    ),\n",
    "             ]\n",
    "nforecast = NeuralForecast(models=models, freq='M')\n",
    "nforecast.fit(df=Y_train_df)\n",
    "Y_hat_df = nforecast.predict(futr_df=futr_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30866cf9-6543-4f6e-802c-febe741b3492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "Y_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\n",
    "plot_df = pd.concat([Y_train_df, Y_hat_df]).set_index('ds')\n",
    "\n",
    "plot_df[['y','LSTM','NBEATSx', 'NHITS']].plot(ax=ax, linewidth=2, marker='o')\n",
    "\n",
    "plt.axvline(cutoff_date, color='red')\n",
    "ax.set_title(f'LI2CO3 Actual and Predicted Plot - Cutoff: {cutoff_date}', fontsize=22)\n",
    "ax.set_ylabel('LI2CO3_PRICE', fontsize=20)\n",
    "ax.set_xlabel('Date', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()\n",
    "\n",
    "# fig1.savefig(f'./data/forecasting_{cutoff_date}.png', dpi=100)\n",
    "fig.savefig(f'./data/forecast_plot_{cutoff_date}_wv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724b4e1-2d8c-4f3e-a5f8-433b9850c9d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_df[['y', 'NHITS']].to_csv(f'./data/forecast_plot_{cutoff_date}_wv.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a7b7d-0c76-4d10-ba18-567112c03448",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0c3b1-9e46-43db-b4e5-b2bf631c573c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd8b962-edb3-4012-81c9-8ea38a116995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
